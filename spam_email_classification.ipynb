{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Email Classification using Transformer-Based Models with LoRA Fine-Tuning\n",
        "\n",
        "## Overview\n",
        "This notebook implements a production-ready spam email classification system using state-of-the-art transformer models with LoRA (Low-Rank Adaptation) fine-tuning. This implementation demonstrates an industry-standard approach that offers superior performance, efficiency, and deployment readiness."
      ],
      "metadata": {
        "id": "YBe4o2GcjvUj"
      },
      "id": "YBe4o2GcjvUj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why LoRA Fine-Tuning Instead of Building from Scratch?\n",
        "\n",
        "### For All Readers (High-Level Explanation)\n",
        "\n",
        "Instead of building a basic sequential neural network from scratch, this project uses **LoRA fine-tuning on pre-trained transformer models**. Here's why this approach makes more sense in real-world applications:\n",
        "\n",
        "**Key Benefits:**\n",
        "- **Faster Development & Training**: Fine-tuning takes hours instead of days, allowing for rapid prototyping and iteration\n",
        "- **Superior Performance**: Pre-trained models have already learned language patterns from billions of text samples, giving us a head start\n",
        "- **Production-Ready**: This approach is used by industry leaders (OpenAI, Google, Meta) for deploying ML models at scale\n",
        "- **Cost-Effective**: Requires significantly less computational resources and training time compared to training from scratch\n",
        "- **Better Generalization**: Pre-trained models handle diverse email patterns and evolving spam tactics more effectively\n",
        "\n",
        "**Real-World Context:**\n",
        "In a production environment, delivering a high-performing model quickly is critical. LoRA fine-tuning allows us to achieve 99%+ accuracy in a fraction of the time it would take to develop and tune a custom architecture, making it the preferred choice for time-sensitive deployments."
      ],
      "metadata": {
        "id": "xBuaQiqrj9KB"
      },
      "id": "xBuaQiqrj9KB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technical Deep Dive (Optional Reading)\n",
        "\n",
        "### For Technical Readers: How LoRA Works Conceptually\n",
        "\n",
        "**What is LoRA?**\n",
        "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable low-rank matrices into each layer. Instead of updating millions of parameters, we only train a small fraction (typically 0.1-1% of total parameters).\n",
        "\n",
        "**How It Works:**\n",
        "- Pre-trained transformer weights remain frozen\n",
        "- Small trainable matrices (rank decomposition) are added to attention layers\n",
        "- During fine-tuning, only these small matrices are updated\n",
        "- The low-rank constraint acts as regularization, preventing overfitting\n",
        "\n",
        "**Why This Matters for Spam Detection:**\n",
        "- **Adaptability**: Spam tactics evolve constantly; LoRA allows quick retraining on new patterns\n",
        "- **Memory Efficiency**: Multiple spam classifiers (different domains/languages) can share the same base model with different LoRA adapters\n",
        "- **Deployment Flexibility**: Easy to update, version, and A/B test different adaptations\n",
        "\n",
        "### Model Selection & Benchmarking Strategy\n",
        "\n",
        "**Models Evaluated:**\n",
        "1. **ELECTRA-base-discriminator**: Efficient pre-training approach, excellent for classification tasks\n",
        "2. **RoBERTa-base**: Robust optimization of BERT, strong language understanding\n",
        "\n",
        "**Why These Models?**\n",
        "These models are the most commonly used base models for spam classification on HuggingFace, demonstrating proven effectiveness in production environments.\n",
        "\n",
        "**Ablation Study:**\n",
        "- Each model tested with LoRA rank 4 and rank 8\n",
        "- Total configurations: 2 models × 2 ranks = 4 experiments\n",
        "- This systematic comparison ensures we select the optimal configuration for accuracy and efficiency"
      ],
      "metadata": {
        "id": "GvPwinMVkEro"
      },
      "id": "GvPwinMVkEro"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Structure\n",
        "\n",
        "This notebook is organized into the following sections:\n",
        "\n",
        "1. **Setup & Configuration** - Environment setup, library imports, and configuration loading\n",
        "2. **Model Download** - Downloading pre-trained models for local use\n",
        "3. **Data Loading & Exploration** - Loading the spam dataset and performing exploratory data analysis (EDA)\n",
        "4. **Data Preprocessing & Splitting** - Text cleaning and stratified train/validation/test split\n",
        "5. **Model Architecture & Training Setup** - LoRA configuration and model initialization\n",
        "6. **Training Execution & Results** - Training loop with logging and checkpointing\n",
        "7. **Model Evaluation & Comparison** - Comprehensive evaluation with multiple metrics and visualizations\n",
        "8. **Conclusion & Key Findings** - Summary of results and model selection\n",
        "\n",
        "Each section contains detailed code with outputs for complete reproducibility."
      ],
      "metadata": {
        "id": "UmuvigTRkIC0"
      },
      "id": "UmuvigTRkIC0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup & Configuration\n",
        "\n",
        "This section handles environment setup, library imports, configuration loading, and seed initialization for reproducibility."
      ],
      "metadata": {
        "id": "NV_4hk5ck5sO"
      },
      "id": "NV_4hk5ck5sO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ceb6ccb-e198-4a4b-b8d4-c954ca08296c",
      "metadata": {
        "id": "5ceb6ccb-e198-4a4b-b8d4-c954ca08296c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "from spacy.cli import download as spacy_download\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from loguru import logger\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ed56fb-9290-447d-9176-1e2e6d2b83ad",
      "metadata": {
        "id": "30ed56fb-9290-447d-9176-1e2e6d2b83ad",
        "outputId": "2a4d4d8b-1a8f-42e7-d042-6c425dc7c1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23:30:29 | INFO | Configuration loaded successfully\n",
            "23:30:29 | INFO | Device: cuda\n",
            "23:30:29 | INFO | Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "SEED = config['data']['random_seed']\n",
        "DEVICE = config['device'] if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)\n",
        "\n",
        "for path_key in config['paths'].values():\n",
        "    Path(path_key).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "log_file = Path(config['paths']['logs']) / 'training.log'\n",
        "logger.remove()\n",
        "logger.add(log_file, level=config['logging']['level'], format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
        "logger.add(sys.stdout, level=config['logging']['level'], format=\"{time:HH:mm:ss} | {level} | {message}\")\n",
        "\n",
        "logger.info(f\"Configuration loaded successfully\")\n",
        "logger.info(f\"Device: {DEVICE}\")\n",
        "logger.info(f\"Random seed: {SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28c5c06-a3a7-4e58-b9a6-534e21e7025c",
      "metadata": {
        "id": "b28c5c06-a3a7-4e58-b9a6-534e21e7025c",
        "outputId": "2bdfab9a-60a3-48ab-ce0a-05864cfef3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:38:40 | INFO | Loaded spaCy model: en_core_web_sm\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    logger.info(\"Downloaded NLTK data\")\n",
        "\n",
        "if config['spacy']['auto_download']:\n",
        "    try:\n",
        "        nlp = spacy.load(config['spacy']['model'])\n",
        "        logger.info(f\"Loaded spaCy model: {config['spacy']['model']}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error Logged: {str(e)}\")\n",
        "        logger.info(f\"Downloading spaCy model: {config['spacy']['model']}\")\n",
        "        spacy_download(config['spacy']['model'])\n",
        "        nlp = spacy.load(config['spacy']['model'])\n",
        "        logger.info(f\"Loaded spaCy model: {config['spacy']['model']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a43755-c2b4-47b6-92bf-7a71d66414bb",
      "metadata": {
        "id": "e4a43755-c2b4-47b6-92bf-7a71d66414bb"
      },
      "outputs": [],
      "source": [
        "def save_plot(filename: str, subdir: str = 'eda'):\n",
        "    save_path = Path(config['paths']['visualizations']) / subdir\n",
        "    save_path.mkdir(parents=True, exist_ok=True)\n",
        "    full_path = save_path / filename\n",
        "    plt.savefig(full_path, dpi=config['visualization']['dpi'], bbox_inches='tight')\n",
        "    plt.close()\n",
        "    logger.info(f\"Saved plot: {full_path}\")\n",
        "    return str(full_path)\n",
        "\n",
        "def save_dataframe_in_multiple_formats(df: pd.DataFrame, name: str, output_dir: str = './data_splits'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    base_path = os.path.join(output_dir, name)\n",
        "    csv_path = f\"{base_path}.csv\"\n",
        "    parquet_path = f\"{base_path}.parquet\"\n",
        "    feather_path = f\"{base_path}.feather\"\n",
        "\n",
        "    print(f\"\\nSaving {name} split...\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"  -> Saved CSV to: {csv_path}\")\n",
        "\n",
        "    df.to_parquet(parquet_path, index=False)\n",
        "    print(f\"  -> Saved Parquet to: {parquet_path}\")\n",
        "\n",
        "    df.to_feather(feather_path)\n",
        "    print(f\"  -> Saved Feather to: {feather_path}\")\n",
        "    del df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Download (Optional - Run Once)\n",
        "\n",
        "This section downloads the pre-trained transformer models from HuggingFace and saves them locally. This is optional if you want to work offline or avoid repeated downloads.\n",
        "\n",
        "**Models to Download:**\n",
        "- [`google/electra-base-discriminator`](https://huggingface.co/google/electra-base-discriminator)\n",
        "- [`FacebookAI/roberta-base`](https://huggingface.co/FacebookAI/roberta-base)\n",
        "\n",
        "**Note:** You can skip this cell if you want to download models automatically during training, or if you're loading models from a different location."
      ],
      "metadata": {
        "id": "ByG4XlqlpYvc"
      },
      "id": "ByG4XlqlpYvc"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAMES = [\"google/electra-base-discriminator\", \"FacebookAI/roberta-base\"]\n",
        "SAVE_DIR = \"./local_models\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "local_paths = {}\n",
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "    print(f\"Downloading and saving: {model_name}\")\n",
        "\n",
        "    model_folder_name = model_name.replace(\"/\", \"_\")\n",
        "    model_dir = os.path.join(SAVE_DIR, model_folder_name)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    tokenizer.save_pretrained(model_dir)\n",
        "    model.save_pretrained(model_dir)\n",
        "\n",
        "    local_paths[model_name] = model_dir\n",
        "\n",
        "print(\"Download complete!\")\n",
        "print(\"\\nLocal model directories:\")\n",
        "for k, v in local_paths.items():\n",
        "    print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "id": "lQK1ntFWpZdQ"
      },
      "id": "lQK1ntFWpZdQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Loading & Exploration (EDA)\n",
        "\n",
        "This section loads the spam email dataset and performs comprehensive exploratory data analysis including:\n",
        "- Dataset overview and statistics\n",
        "- Class distribution analysis\n",
        "- Text length analysis\n",
        "- N-gram analysis (unigrams and bigrams)\n",
        "- Word clouds visualization\n",
        "- Special character and pattern analysis\n",
        "- Linguistic analysis using spaCy (POS tags and named entities)"
      ],
      "metadata": {
        "id": "eFtbWtLVk-St"
      },
      "id": "eFtbWtLVk-St"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d386bf1c-f000-4740-adf8-01462fbc1b01",
      "metadata": {
        "id": "d386bf1c-f000-4740-adf8-01462fbc1b01",
        "outputId": "8b4c8930-75a1-45e1-a91f-dc9ddbf7a07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23:30:32 | INFO | Loading dataset...\n",
            "23:30:33 | INFO | Dataset loaded: 83448 rows, 2 columns\n",
            "============================================================\n",
            "DATASET OVERVIEW\n",
            "============================================================\n",
            "Shape: (83448, 2)\n",
            "\n",
            "Columns: ['label', 'text']\n",
            "\n",
            "Data types:\n",
            "label     int64\n",
            "text     object\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "label    0\n",
            "text     0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate rows: 0\n",
            "Duplicate texts: 2\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "1    43910\n",
            "0    39538\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample data:\n",
            "   label                                               text\n",
            "0      1  ounce feather bowl hummingbird opec moment ala...\n",
            "1      1  wulvob get your medircations online qnb ikud v...\n",
            "2      0   computer connection from cnn com wednesday es...\n",
            "3      1  university degree obtain a prosperous future m...\n",
            "4      0  thanks for all your answers guys i know i shou...\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Loading dataset...\")\n",
        "df = pd.read_csv(config['data']['path'])\n",
        "logger.info(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
        "print(f\"Duplicate texts: {df[config['data']['text_column']].duplicated().sum()}\")\n",
        "print(f\"\\nLabel distribution:\\n{df[config['data']['label_column']].value_counts()}\")\n",
        "print(f\"\\nSample data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4615071-29e4-43cf-b972-77232308f7cc",
      "metadata": {
        "id": "c4615071-29e4-43cf-b972-77232308f7cc",
        "outputId": "ec556c76-c15b-46df-a4e7-b2999e461757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:15:17 | INFO | Computing text statistics...\n",
            "\n",
            "============================================================\n",
            "TEXT LENGTH STATISTICS\n",
            "============================================================\n",
            "         text_length     word_count  avg_word_length  unique_words\n",
            "count   83448.000000   83448.000000     83448.000000  83448.000000\n",
            "mean     1662.952725     282.811775         4.918200    127.848193\n",
            "std      4178.578068     724.818152         1.773719    145.888237\n",
            "min         1.000000       1.000000         1.000000      1.000000\n",
            "25%       449.000000      80.000000         4.000000     54.000000\n",
            "50%       879.000000     152.000000         4.762500     92.000000\n",
            "75%      1861.000000     312.000000         5.489655    156.000000\n",
            "max    598705.000000  101984.000000       175.500000   5182.000000\n",
            "\n",
            "============================================================\n",
            "STATISTICS BY CLASS\n",
            "============================================================\n",
            "\n",
            "SPAM (label=1):\n",
            "         text_length    word_count  avg_word_length  unique_words\n",
            "count   43910.000000  43910.000000     43910.000000  43910.000000\n",
            "mean     1249.887247    208.754634         5.149023    111.648463\n",
            "std      1978.631507    338.269557         2.164387    107.537529\n",
            "min         1.000000      1.000000         1.000000      1.000000\n",
            "25%       360.000000     61.000000         4.154558     44.000000\n",
            "50%       704.000000    122.000000         4.853333     80.000000\n",
            "75%      1510.000000    249.000000         5.687500    140.000000\n",
            "max    144087.000000  25393.000000       175.500000   1965.000000\n",
            "\n",
            "HAM (label=0):\n",
            "         text_length     word_count  avg_word_length  unique_words\n",
            "count   39538.000000   39538.000000     39538.000000  39538.000000\n",
            "mean     2121.693814     365.057944         4.661854    145.839243\n",
            "std      5666.075286     984.324750         1.145711    177.376895\n",
            "min         1.000000       1.000000         1.000000      1.000000\n",
            "25%       579.000000     106.000000         3.794872     65.000000\n",
            "50%      1139.000000     200.000000         4.683708    105.000000\n",
            "75%      2174.000000     378.000000         5.293060    169.000000\n",
            "max    598705.000000  101984.000000        14.300813   5182.000000\n",
            "09:15:26 | INFO | Saved plot: visualizations/eda/text_statistics.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/text_statistics.png'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Computing text statistics...\")\n",
        "\n",
        "df['text_length'] = df[config['data']['text_column']].str.len()\n",
        "df['word_count'] = df[config['data']['text_column']].str.split().str.len()\n",
        "df['avg_word_length'] = df.apply(lambda row: np.mean([len(word) for word in row[config['data']['text_column']].split()]) if row['word_count'] > 0 else 0, axis=1)\n",
        "df['unique_words'] = df[config['data']['text_column']].apply(lambda x: len(set(x.lower().split())))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEXT LENGTH STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(df[['text_length', 'word_count', 'avg_word_length', 'unique_words']].describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTICS BY CLASS\")\n",
        "print(\"=\"*60)\n",
        "for label in df[config['data']['label_column']].unique():\n",
        "    label_name = 'SPAM' if label == 1 else 'HAM'\n",
        "    print(f\"\\n{label_name} (label={label}):\")\n",
        "    print(df[df[config['data']['label_column']] == label][['text_length', 'word_count', 'avg_word_length', 'unique_words']].describe())\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "axes[0, 0].hist([df[df[config['data']['label_column']]==1]['text_length'], df[df[config['data']['label_column']]==0]['text_length']],  label=['Spam', 'Ham'], bins=50, alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Text Length (characters)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Text Length Distribution')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].hist([df[df[config['data']['label_column']]==1]['word_count'], df[df[config['data']['label_column']]==0]['word_count']], label=['Spam', 'Ham'], bins=50, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Word Count')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Word Count Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].boxplot([df[df[config['data']['label_column']]==1]['avg_word_length'], df[df[config['data']['label_column']]==0]['avg_word_length']], labels=['Spam', 'Ham'])\n",
        "axes[1, 0].set_ylabel('Average Word Length')\n",
        "axes[1, 0].set_title('Average Word Length by Class')\n",
        "\n",
        "axes[1, 1].hist([df[df[config['data']['label_column']]==1]['unique_words'], df[df[config['data']['label_column']]==0]['unique_words']], label=['Spam', 'Ham'], bins=50, alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Unique Words')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Unique Words Distribution')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('text_statistics.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb50d1a-6840-457d-be92-c0e0284957a6",
      "metadata": {
        "id": "0cb50d1a-6840-457d-be92-c0e0284957a6",
        "outputId": "0636a265-69d0-4f97-b5d0-458fd0e3fd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:15:26 | INFO | Analyzing token lengths with DeBERTa tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TOKEN LENGTH STATISTICS\n",
            "============================================================\n",
            "count     83448.000000\n",
            "mean        379.136888\n",
            "std        1032.527373\n",
            "min           3.000000\n",
            "25%         105.000000\n",
            "50%         204.000000\n",
            "75%         423.000000\n",
            "max      182763.000000\n",
            "Name: token_length, dtype: float64\n",
            "\n",
            "Texts > 512 tokens: 15831 (18.97%)\n",
            "Texts > 1024 tokens: 4648 (5.57%)\n",
            "Texts > 2048 tokens: 1306 (1.57%)\n",
            "\n",
            "SPAM > 512 tokens: 6287\n",
            "\n",
            "HAM > 512 tokens: 9544\n",
            "09:16:23 | INFO | Saved plot: visualizations/eda/token_length_analysis.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/token_length_analysis.png'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Analyzing token lengths with tokenizer...\")\n",
        "\n",
        "tokenizer_test = AutoTokenizer.from_pretrained(config['models']['names'][0])\n",
        "df['token_length'] = df[config['data']['text_column']].apply(\n",
        "    lambda x: len(tokenizer_test.encode(x, add_special_tokens=True, truncation=False))\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOKEN LENGTH STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(df['token_length'].describe())\n",
        "\n",
        "print(f\"\\nTexts > 512 tokens: {(df['token_length'] > 512).sum()} ({(df['token_length'] > 512).sum() / len(df) * 100:.2f}%)\")\n",
        "print(f\"Texts > 1024 tokens: {(df['token_length'] > 1024).sum()} ({(df['token_length'] > 1024).sum() / len(df) * 100:.2f}%)\")\n",
        "print(f\"Texts > 2048 tokens: {(df['token_length'] > 2048).sum()} ({(df['token_length'] > 2048).sum() / len(df) * 100:.2f}%)\")\n",
        "\n",
        "for label in df[config['data']['label_column']].unique():\n",
        "    label_name = 'SPAM' if label == 1 else 'HAM'\n",
        "    print(f\"\\n{label_name} > 512 tokens: {(df[df[config['data']['label_column']]==label]['token_length'] > 512).sum()}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "axes[0, 0].hist(df['token_length'].clip(upper=2000), bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(x=512, color='red', linestyle='--', linewidth=2, label='512 token limit')\n",
        "axes[0, 0].set_xlabel('Token Length (capped at 2000)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Overall Token Length Distribution')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "spam_tokens = df[df[config['data']['label_column']]==1]['token_length']\n",
        "ham_tokens = df[df[config['data']['label_column']]==0]['token_length']\n",
        "axes[0, 1].hist([spam_tokens.clip(upper=2000), ham_tokens.clip(upper=2000)], bins=50, label=['Spam', 'Ham'], alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].axvline(x=512, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Token Length (capped at 2000)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Token Length: Spam vs Ham')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].boxplot([spam_tokens, ham_tokens], labels=['Spam', 'Ham'])\n",
        "axes[1, 0].axhline(y=512, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 0].set_ylabel('Token Length')\n",
        "axes[1, 0].set_title('Token Length Distribution by Class')\n",
        "\n",
        "axes[1, 1].hist(df['token_length'], bins=100, cumulative=True, density=True, alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].axvline(x=512, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 1].axhline(y=0.95, color='green', linestyle='--', linewidth=1, alpha=0.5)\n",
        "axes[1, 1].set_xlabel('Token Length')\n",
        "axes[1, 1].set_ylabel('Cumulative Probability')\n",
        "axes[1, 1].set_title('Cumulative Distribution')\n",
        "axes[1, 1].set_xlim(0, 2000)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('token_length_analysis.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259614ec-ab1a-4caf-b36c-45643507db09",
      "metadata": {
        "id": "259614ec-ab1a-4caf-b36c-45643507db09",
        "outputId": "a744fae8-69cd-4a9a-fbcd-263d76e06f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:17:06 | INFO | Saved plot: visualizations/eda/class_distribution.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/class_distribution.png'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "label_counts = df[config['data']['label_column']].value_counts()\n",
        "axes[0].bar(['Ham (0)', 'Spam (1)'], [label_counts[0], label_counts[1]], color=['#2D8E4D', '#E67E22'])\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Label Distribution (Counts)')\n",
        "for i, v in enumerate([label_counts[0], label_counts[1]]):\n",
        "    axes[0].text(i, v + 500, str(v), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "axes[1].pie([label_counts[0], label_counts[1]], labels=['Ham (0)', 'Spam (1)'], autopct='%1.1f%%', colors=['#2D8E4D', '#E67E22'], startangle=90)\n",
        "axes[1].set_title('Label Distribution (Percentage)')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('class_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26090477-0fe4-4444-b9b2-1e95b2e8cf98",
      "metadata": {
        "id": "26090477-0fe4-4444-b9b2-1e95b2e8cf98",
        "outputId": "bfaa020f-2d31-479b-b1ae-fa1f980a9054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:17:08 | INFO | Performing N-gram analysis...\n",
            "\n",
            "============================================================\n",
            "TOP 20 UNIGRAMS\n",
            "============================================================\n",
            "\n",
            "SPAM:\n",
            "escapenumber: 333179\n",
            "escapelong: 188102\n",
            "com: 29001\n",
            "http: 27754\n",
            "per: 26141\n",
            "pills: 23128\n",
            "escapenumbermg: 20541\n",
            "price: 18723\n",
            "company: 15859\n",
            "one: 15853\n",
            "save: 15005\n",
            "may: 14662\n",
            "item: 14472\n",
            "time: 12351\n",
            "please: 12330\n",
            "get: 11986\n",
            "new: 11385\n",
            "money: 11378\n",
            "information: 11158\n",
            "see: 10819\n",
            "\n",
            "HAM:\n",
            "escapenumber: 798212\n",
            "http: 54360\n",
            "enron: 52856\n",
            "org: 42530\n",
            "com: 40658\n",
            "escapelong: 38909\n",
            "ect: 34743\n",
            "help: 32512\n",
            "samba: 30703\n",
            "list: 28223\n",
            "www: 28129\n",
            "please: 26786\n",
            "new: 25642\n",
            "would: 25381\n",
            "source: 25127\n",
            "data: 21931\n",
            "may: 21597\n",
            "stat: 21220\n",
            "ethz: 20493\n",
            "html: 19548\n",
            "\n",
            "============================================================\n",
            "TOP 15 BIGRAMS\n",
            "============================================================\n",
            "\n",
            "SPAM:\n",
            "escapelong escapelong: 156976\n",
            "escapenumber escapenumber: 144250\n",
            "escapenumber per: 24186\n",
            "pills escapenumbermg: 19809\n",
            "escapenumbermg escapenumber: 17712\n",
            "escapenumber pills: 16672\n",
            "per item: 13956\n",
            "save escapenumber: 10386\n",
            "price escapenumber: 9724\n",
            "http www: 7147\n",
            "item save: 7122\n",
            "retail price: 7072\n",
            "escapenumber adobe: 4804\n",
            "escapenumber escapelong: 4297\n",
            "low escapenumber: 3787\n",
            "\n",
            "HAM:\n",
            "escapenumber escapenumber: 467685\n",
            "http www: 26060\n",
            "samba escapenumber: 18365\n",
            "posting guide: 18267\n",
            "escapenumber source: 17568\n",
            "branches samba: 16321\n",
            "hou ect: 15931\n",
            "ect ect: 15493\n",
            "mailing list: 14683\n",
            "escapenumberd escapenumberd: 14378\n",
            "escapenumber escapelong: 12912\n",
            "mailman listinfo: 12634\n",
            "math ethz: 11341\n",
            "stat math: 11300\n",
            "help stat: 10040\n",
            "09:17:29 | INFO | Saved plot: visualizations/eda/ngram_analysis.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/ngram_analysis.png'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Performing N-gram analysis...\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def get_ngrams(texts, n=1, top_k=20):\n",
        "    all_ngrams = []\n",
        "    for text in texts:\n",
        "        words = [word.lower() for word in text.split() if word.lower() not in stop_words and len(word) > 2]\n",
        "        all_ngrams.extend(list(ngrams(words, n)))\n",
        "    return Counter(all_ngrams).most_common(top_k)\n",
        "\n",
        "spam_texts = df[df[config['data']['label_column']]==1][config['data']['text_column']].tolist()\n",
        "ham_texts = df[df[config['data']['label_column']]==0][config['data']['text_column']].tolist()\n",
        "\n",
        "spam_unigrams = get_ngrams(spam_texts, n=1, top_k=20)\n",
        "ham_unigrams = get_ngrams(ham_texts, n=1, top_k=20)\n",
        "spam_bigrams = get_ngrams(spam_texts, n=2, top_k=15)\n",
        "ham_bigrams = get_ngrams(ham_texts, n=2, top_k=15)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 UNIGRAMS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSPAM:\")\n",
        "for ngram, count in spam_unigrams:\n",
        "    print(f\"{ngram[0]}: {count}\")\n",
        "print(\"\\nHAM:\")\n",
        "for ngram, count in ham_unigrams:\n",
        "    print(f\"{ngram[0]}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 15 BIGRAMS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSPAM:\")\n",
        "for ngram, count in spam_bigrams:\n",
        "    print(f\"{' '.join(ngram)}: {count}\")\n",
        "print(\"\\nHAM:\")\n",
        "for ngram, count in ham_bigrams:\n",
        "    print(f\"{' '.join(ngram)}: {count}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "spam_uni_words = [ngram[0][0] for ngram in spam_unigrams]\n",
        "spam_uni_counts = [count for ngram, count in spam_unigrams]\n",
        "axes[0, 0].barh(spam_uni_words[::-1], spam_uni_counts[::-1], color='#E67E22')\n",
        "axes[0, 0].set_xlabel('Count')\n",
        "axes[0, 0].set_title('Top 20 Unigrams - SPAM')\n",
        "\n",
        "ham_uni_words = [ngram[0][0] for ngram in ham_unigrams]\n",
        "ham_uni_counts = [count for ngram, count in ham_unigrams]\n",
        "axes[0, 1].barh(ham_uni_words[::-1], ham_uni_counts[::-1], color='#2D8E4D')\n",
        "axes[0, 1].set_xlabel('Count')\n",
        "axes[0, 1].set_title('Top 20 Unigrams - HAM')\n",
        "\n",
        "spam_bi_words = [' '.join(ngram[0]) for ngram in spam_bigrams]\n",
        "spam_bi_counts = [count for ngram, count in spam_bigrams]\n",
        "axes[1, 0].barh(spam_bi_words[::-1], spam_bi_counts[::-1], color='#E67E22')\n",
        "axes[1, 0].set_xlabel('Count')\n",
        "axes[1, 0].set_title('Top 15 Bigrams - SPAM')\n",
        "\n",
        "ham_bi_words = [' '.join(ngram[0]) for ngram in ham_bigrams]\n",
        "ham_bi_counts = [count for ngram, count in ham_bigrams]\n",
        "axes[1, 1].barh(ham_bi_words[::-1], ham_bi_counts[::-1], color='#2D8E4D')\n",
        "axes[1, 1].set_xlabel('Count')\n",
        "axes[1, 1].set_title('Top 15 Bigrams - HAM')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('ngram_analysis.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c49a28-94c2-4bca-96f8-f86b21b3af05",
      "metadata": {
        "id": "10c49a28-94c2-4bca-96f8-f86b21b3af05",
        "outputId": "ce0b728b-a76b-4fad-cc2c-a21a8b4fdacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:17:29 | INFO | Generating word clouds...\n",
            "09:18:10 | INFO | Saved plot: visualizations/eda/wordclouds.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/wordclouds.png'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Generating word clouds...\")\n",
        "\n",
        "spam_text = ' '.join(spam_texts)\n",
        "ham_text = ' '.join(ham_texts)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "wc_spam = WordCloud(width=800, height=400, background_color='white', colormap='Reds', stopwords=stop_words, max_words=100).generate(spam_text)\n",
        "axes[0].imshow(wc_spam, interpolation='bilinear')\n",
        "axes[0].set_title('Word Cloud - SPAM', fontsize=16, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "wc_ham = WordCloud(width=800, height=400, background_color='white', colormap='Greens', stopwords=stop_words, max_words=100).generate(ham_text)\n",
        "axes[1].imshow(wc_ham, interpolation='bilinear')\n",
        "axes[1].set_title('Word Cloud - HAM', fontsize=16, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('wordclouds.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7eb857-ff5c-4e9e-8bd0-5de23f414708",
      "metadata": {
        "id": "ce7eb857-ff5c-4e9e-8bd0-5de23f414708",
        "outputId": "aab094b9-9de8-41f7-fe6c-eaa206b8c5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:18:10 | INFO | Analyzing special characters and patterns...\n",
            "\n",
            "============================================================\n",
            "SPECIAL CHARACTER ANALYSIS BY CLASS\n",
            "============================================================\n",
            "\n",
            "HAM (label=0):\n",
            "       has_url  has_email     has_phone  dollar_count  percent_count  \\\n",
            "count  39538.0    39538.0  39538.000000  39538.000000   39538.000000   \n",
            "mean       0.0        0.0      0.019273      0.414917       0.088851   \n",
            "std        0.0        0.0      0.137483      5.409788       1.111040   \n",
            "min        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "25%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "50%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "75%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "max        0.0        0.0      1.000000    349.000000      90.000000   \n",
            "\n",
            "       exclamation_count  caps_ratio  punctuation_density  has_html  \n",
            "count       39538.000000     39538.0         39538.000000   39538.0  \n",
            "mean            0.251758         0.0             0.002593       0.0  \n",
            "std             2.285224         0.0             0.008227       0.0  \n",
            "min             0.000000         0.0             0.000000       0.0  \n",
            "25%             0.000000         0.0             0.000000       0.0  \n",
            "50%             0.000000         0.0             0.000000       0.0  \n",
            "75%             0.000000         0.0             0.002629       0.0  \n",
            "max            46.000000         0.0             1.000000       0.0  \n",
            "\n",
            "SPAM (label=1):\n",
            "       has_url  has_email     has_phone  dollar_count  percent_count  \\\n",
            "count  43910.0    43910.0  43910.000000  43910.000000   43910.000000   \n",
            "mean       0.0        0.0      0.003507      0.430380       0.186313   \n",
            "std        0.0        0.0      0.059118      3.549748       2.592781   \n",
            "min        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "25%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "50%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "75%        0.0        0.0      0.000000      0.000000       0.000000   \n",
            "max        0.0        0.0      1.000000    487.000000     183.000000   \n",
            "\n",
            "       exclamation_count    caps_ratio  punctuation_density  has_html  \n",
            "count       43910.000000  43910.000000         43910.000000   43910.0  \n",
            "mean            0.536825      0.000006             0.002234       0.0  \n",
            "std             2.249173      0.000438             0.007166       0.0  \n",
            "min             0.000000      0.000000             0.000000       0.0  \n",
            "25%             0.000000      0.000000             0.000000       0.0  \n",
            "50%             0.000000      0.000000             0.000000       0.0  \n",
            "75%             0.000000      0.000000             0.000480       0.0  \n",
            "max            88.000000      0.047710             0.423263       0.0  \n",
            "09:18:29 | INFO | Saved plot: visualizations/eda/special_character_analysis.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/special_character_analysis.png'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Analyzing special characters and patterns...\")\n",
        "\n",
        "df['has_url'] = df[config['data']['text_column']].str.contains(r'http[s]?://|www\\.', regex=True, case=False).astype(int)\n",
        "df['has_email'] = df[config['data']['text_column']].str.contains(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', regex=True).astype(int)\n",
        "df['has_phone'] = df[config['data']['text_column']].str.contains(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', regex=True).astype(int)\n",
        "df['dollar_count'] = df[config['data']['text_column']].str.count(r'\\$')\n",
        "df['percent_count'] = df[config['data']['text_column']].str.count(r'%')\n",
        "df['exclamation_count'] = df[config['data']['text_column']].str.count(r'!')\n",
        "df['caps_ratio'] = df[config['data']['text_column']].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
        "df['punctuation_density'] = df[config['data']['text_column']].apply(lambda x: sum(1 for c in x if c in '!@#$%^&*()') / len(x) if len(x) > 0 else 0)\n",
        "df['has_html'] = df[config['data']['text_column']].str.contains(r'<[^>]+>', regex=True).astype(int)\n",
        "\n",
        "special_char_features = ['has_url', 'has_email', 'has_phone', 'dollar_count', 'percent_count', 'exclamation_count', 'caps_ratio', 'punctuation_density', 'has_html']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SPECIAL CHARACTER ANALYSIS BY CLASS\")\n",
        "print(\"=\"*60)\n",
        "for label in [0, 1]:\n",
        "    label_name = 'HAM' if label == 0 else 'SPAM'\n",
        "    print(f\"\\n{label_name} (label={label}):\")\n",
        "    print(df[df[config['data']['label_column']]==label][special_char_features].describe())\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(special_char_features):\n",
        "    spam_vals = df[df[config['data']['label_column']]==1][feature]\n",
        "    ham_vals = df[df[config['data']['label_column']]==0][feature]\n",
        "    axes[idx].hist([spam_vals, ham_vals], label=['Spam', 'Ham'], bins=30, alpha=0.7)\n",
        "    axes[idx].set_xlabel(feature.replace('_', ' ').title())\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
        "    axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('special_character_analysis.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f4b0a8-b7f1-41ba-a31e-31e8259714fe",
      "metadata": {
        "id": "c7f4b0a8-b7f1-41ba-a31e-31e8259714fe",
        "outputId": "fba4eed4-da89-4002-dc8f-d54ca693aaf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:18:29 | INFO | Performing linguistic analysis with spaCy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing with spaCy: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:21<00:00, 47.52it/s]\n",
            "Processing with spaCy: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 40.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TOP NAMED ENTITIES\n",
            "============================================================\n",
            "\n",
            "SPAM:\n",
            "CARDINAL: 1486\n",
            "PERSON: 966\n",
            "ORG: 823\n",
            "DATE: 775\n",
            "MONEY: 430\n",
            "GPE: 360\n",
            "NORP: 226\n",
            "TIME: 140\n",
            "PERCENT: 98\n",
            "ORDINAL: 82\n",
            "\n",
            "HAM:\n",
            "PERSON: 2316\n",
            "CARDINAL: 1470\n",
            "ORG: 1460\n",
            "DATE: 1448\n",
            "GPE: 513\n",
            "TIME: 350\n",
            "NORP: 201\n",
            "MONEY: 148\n",
            "ORDINAL: 118\n",
            "QUANTITY: 65\n",
            "\n",
            "============================================================\n",
            "TOP POS TAGS\n",
            "============================================================\n",
            "\n",
            "SPAM:\n",
            "NOUN: 29120\n",
            "PROPN: 13005\n",
            "VERB: 12028\n",
            "PUNCT: 9133\n",
            "ADJ: 8753\n",
            "ADP: 8587\n",
            "PRON: 8128\n",
            "DET: 6257\n",
            "AUX: 4717\n",
            "SPACE: 4599\n",
            "\n",
            "HAM:\n",
            "NOUN: 33178\n",
            "PROPN: 21545\n",
            "VERB: 14654\n",
            "PUNCT: 13691\n",
            "ADP: 10864\n",
            "PRON: 8083\n",
            "DET: 8078\n",
            "ADJ: 7607\n",
            "AUX: 6274\n",
            "SPACE: 5033\n",
            "09:19:15 | INFO | Saved plot: visualizations/eda/linguistic_analysis.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/eda/linguistic_analysis.png'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Performing linguistic analysis with spaCy...\")\n",
        "\n",
        "def analyze_with_spacy(texts, sample_size=1000):\n",
        "    sampled_texts = random.sample(texts, min(sample_size, len(texts)))\n",
        "\n",
        "    entities = []\n",
        "    pos_tags = []\n",
        "\n",
        "    for text in tqdm(sampled_texts, desc=\"Processing with spaCy\"):\n",
        "        doc = nlp(text[:1000])\n",
        "        entities.extend([ent.label_ for ent in doc.ents])\n",
        "        pos_tags.extend([token.pos_ for token in doc])\n",
        "\n",
        "    return Counter(entities), Counter(pos_tags)\n",
        "\n",
        "spam_entities, spam_pos = analyze_with_spacy(spam_texts)\n",
        "ham_entities, ham_pos = analyze_with_spacy(ham_texts)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP NAMED ENTITIES\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSPAM:\")\n",
        "for ent, count in spam_entities.most_common(10):\n",
        "    print(f\"{ent}: {count}\")\n",
        "print(\"\\nHAM:\")\n",
        "for ent, count in ham_entities.most_common(10):\n",
        "    print(f\"{ent}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP POS TAGS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSPAM:\")\n",
        "for pos, count in spam_pos.most_common(10):\n",
        "    print(f\"{pos}: {count}\")\n",
        "print(\"\\nHAM:\")\n",
        "for pos, count in ham_pos.most_common(10):\n",
        "    print(f\"{pos}: {count}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "spam_ent_labels = [ent for ent, _ in spam_entities.most_common(10)]\n",
        "spam_ent_counts = [count for _, count in spam_entities.most_common(10)]\n",
        "axes[0, 0].barh(spam_ent_labels[::-1], spam_ent_counts[::-1], color='#E67E22')\n",
        "axes[0, 0].set_xlabel('Count')\n",
        "axes[0, 0].set_title('Top Named Entities - SPAM')\n",
        "\n",
        "ham_ent_labels = [ent for ent, _ in ham_entities.most_common(10)]\n",
        "ham_ent_counts = [count for _, count in ham_entities.most_common(10)]\n",
        "axes[0, 1].barh(ham_ent_labels[::-1], ham_ent_counts[::-1], color='#2D8E4D')\n",
        "axes[0, 1].set_xlabel('Count')\n",
        "axes[0, 1].set_title('Top Named Entities - HAM')\n",
        "\n",
        "spam_pos_labels = [pos for pos, _ in spam_pos.most_common(10)]\n",
        "spam_pos_counts = [count for _, count in spam_pos.most_common(10)]\n",
        "axes[1, 0].bar(spam_pos_labels, spam_pos_counts, color='#E67E22')\n",
        "axes[1, 0].set_xlabel('POS Tag')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_title('Top POS Tags - SPAM')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "ham_pos_labels = [pos for pos, _ in ham_pos.most_common(10)]\n",
        "ham_pos_counts = [count for _, count in ham_pos.most_common(10)]\n",
        "axes[1, 1].bar(ham_pos_labels, ham_pos_counts, color='#2D8E4D')\n",
        "axes[1, 1].set_xlabel('POS Tag')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].set_title('Top POS Tags - HAM')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('linguistic_analysis.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data Preprocessing & Splitting\n",
        "\n",
        "This section prepares the data for training by:\n",
        "- Cleaning and preprocessing text (if needed)\n",
        "- Performing stratified train/validation/test split (80/10/10)\n",
        "- Ensuring balanced class distribution across all splits\n",
        "- Saving splits in multiple formats (CSV, Parquet, Feather)\n",
        "\n",
        "**Design Choice: Stratified Split**\n",
        "We use stratified splitting to ensure equal class distribution (spam vs ham) across train, validation, and test sets. This is critical for:\n",
        "- Preventing class imbalance in any split\n",
        "- Ensuring fair evaluation metrics\n",
        "- Maintaining consistent model performance across different data subsets"
      ],
      "metadata": {
        "id": "-zOAPlG3lLEQ"
      },
      "id": "-zOAPlG3lLEQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d98aba9-8b71-43ad-8017-bef0c7517111",
      "metadata": {
        "id": "2d98aba9-8b71-43ad-8017-bef0c7517111",
        "outputId": "b7b6c5d5-6955-44e7-c1ae-24b5f789c69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23:30:45 | INFO | Starting data preprocessing...\n",
            "23:30:45 | INFO | Dataset shape before preprocessing: (83448, 2)\n",
            "23:30:45 | INFO | Dataset is clean, no preprocessing needed\n",
            "23:30:45 | INFO | Final dataset shape: (83448, 2)\n",
            "23:30:45 | INFO | Splitting dataset into train/val/test...\n",
            "23:30:45 | INFO | Train set: 66758 samples\n",
            "23:30:45 | INFO | Validation set: 8345 samples\n",
            "23:30:45 | INFO | Test set: 8345 samples\n",
            "\n",
            "============================================================\n",
            "DATASET SPLIT SUMMARY\n",
            "============================================================\n",
            "Train: 66758 (80.0%)\n",
            "Val:   8345 (10.0%)\n",
            "Test:  8345 (10.0%)\n",
            "\n",
            "Label distribution in splits:\n",
            "Train - Spam: 35128 (52.6%)\n",
            "Val   - Spam: 4391 (52.6%)\n",
            "Test  - Spam: 4391 (52.6%)\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Starting data preprocessing...\")\n",
        "\n",
        "df_clean = df[[config['data']['text_column'], config['data']['label_column']]].copy()\n",
        "logger.info(f\"Dataset shape before preprocessing: {df_clean.shape}\")\n",
        "\n",
        "logger.info(\"Dataset is clean, no preprocessing needed\")\n",
        "logger.info(f\"Final dataset shape: {df_clean.shape}\")\n",
        "\n",
        "logger.info(\"Splitting dataset into train/val/test...\")\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df_clean,\n",
        "    test_size=config['data']['test_size'] + config['data']['val_size'],\n",
        "    random_state=config['data']['random_seed'],\n",
        "    stratify=df_clean[config['data']['label_column']]\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=config['data']['test_size'] / (config['data']['test_size'] + config['data']['val_size']),\n",
        "    random_state=config['data']['random_seed'],\n",
        "    stratify=temp_df[config['data']['label_column']]\n",
        ")\n",
        "\n",
        "logger.info(f\"Train set: {len(train_df)} samples\")\n",
        "logger.info(f\"Validation set: {len(val_df)} samples\")\n",
        "logger.info(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET SPLIT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train: {len(train_df)} ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Val:   {len(val_df)} ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Test:  {len(test_df)} ({len(test_df)/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nLabel distribution in splits:\")\n",
        "print(f\"Train - Spam: {train_df[config['data']['label_column']].sum()} ({train_df[config['data']['label_column']].sum()/len(train_df)*100:.1f}%)\")\n",
        "print(f\"Val   - Spam: {val_df[config['data']['label_column']].sum()} ({val_df[config['data']['label_column']].sum()/len(val_df)*100:.1f}%)\")\n",
        "print(f\"Test  - Spam: {test_df[config['data']['label_column']].sum()} ({test_df[config['data']['label_column']].sum()/len(test_df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e5e9a7-a389-47cb-9ae1-b9a0baf0624d",
      "metadata": {
        "id": "86e5e9a7-a389-47cb-9ae1-b9a0baf0624d",
        "outputId": "86e36692-6519-443a-f8f0-6e93f772a596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving train split...\n",
            "  -> Saved CSV to: ./data_splits/train.csv\n",
            "  -> Saved Parquet to: ./data_splits/train.parquet\n",
            "  -> Saved Feather to: ./data_splits/train.feather\n",
            "\n",
            "Saving val split...\n",
            "  -> Saved CSV to: ./data_splits/val.csv\n",
            "  -> Saved Parquet to: ./data_splits/val.parquet\n",
            "  -> Saved Feather to: ./data_splits/val.feather\n",
            "\n",
            "Saving test split...\n",
            "  -> Saved CSV to: ./data_splits/test.csv\n",
            "  -> Saved Parquet to: ./data_splits/test.parquet\n",
            "  -> Saved Feather to: ./data_splits/test.feather\n"
          ]
        }
      ],
      "source": [
        "save_dataframe_in_multiple_formats(train_df, 'train')\n",
        "save_dataframe_in_multiple_formats(val_df, 'val')\n",
        "save_dataframe_in_multiple_formats(test_df, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Architecture & Training Setup\n",
        "\n",
        "This section defines:\n",
        "- Custom PyTorch Dataset class for tokenization and data loading\n",
        "- Training and evaluation functions with mixed precision\n",
        "- LoRA (Low-Rank Adaptation) configuration\n",
        "- Model training loop with early stopping\n",
        "\n",
        "**Design Choices:**\n",
        "\n",
        "**Why These Models?**\n",
        "- **ELECTRA-base-discriminator**: Efficient pre-training approach optimized for classification tasks\n",
        "- **RoBERTa-base**: Robust optimization of BERT with proven performance on text classification\n",
        "- Both models are the most commonly used base models for spam classification on HuggingFace\n",
        "\n",
        "**Hyperparameter Rationale:**\n",
        "- **2 Epochs**: Sufficient for fine-tuning pre-trained models; prevents overfitting while allowing adaptation to spam patterns\n",
        "- **Learning Rate (2e-4)**: Standard for LoRA fine-tuning, balances convergence speed and stability\n",
        "- **Batch Size (16)**: Optimized for GPU memory efficiency with automatic reduction if OOM occurs\n",
        "- **LoRA Ranks (4 and 8)**: Ablation study to find optimal parameter efficiency vs performance trade-off\n",
        "\n",
        "**Why Precision Matters for Spam Detection:**\n",
        "Precision is prioritized because:\n",
        "- False positives (legitimate emails marked as spam) are more costly than false negatives\n",
        "- Spam tactics evolve constantly; new patterns may not match training data exactly\n",
        "- High precision ensures user trust in the classification system"
      ],
      "metadata": {
        "id": "pXqlHdktlUA0"
      },
      "id": "pXqlHdktlUA0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e8c4e5-e8c0-4014-9ddc-92208c00bff6",
      "metadata": {
        "id": "08e8c4e5-e8c0-4014-9ddc-92208c00bff6"
      },
      "outputs": [],
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, tb_writer, global_step):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['training']['gradient_clipping'])\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        tb_writer.add_scalar('Train/BatchLoss', loss.item(), global_step)\n",
        "        tb_writer.add_scalar('Train/LearningRate', scheduler.get_last_lr()[0], global_step)\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        global_step += 1\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, global_step\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9766aa4-6861-4972-89e0-bddfdab438c2",
      "metadata": {
        "id": "f9766aa4-6861-4972-89e0-bddfdab438c2"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, lora_rank, train_loader, val_loader, device):\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    run_name = f\"{model_short_name}_r{lora_rank}\"\n",
        "\n",
        "    logger.info(f\"Starting training: {run_name}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        problem_type=\"single_label_classification\"\n",
        "    )\n",
        "\n",
        "    lora_alpha = lora_rank * 2 if config['lora']['alpha'] == 'auto' else config['lora']['alpha']\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=lora_rank,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=config['lora']['dropout'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=\"none\"\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "    model = model.to(device)\n",
        "\n",
        "    logger.info(f\"Model loaded: {model_name} with LoRA rank={lora_rank}, alpha={lora_alpha}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=float(config['training']['learning_rate']),\n",
        "        weight_decay=config['training']['weight_decay']\n",
        "    )\n",
        "\n",
        "    total_steps = len(train_loader) * config['training']['epochs']\n",
        "    warmup_steps = int(total_steps * config['training']['warmup_ratio'])\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    tb_writer = SummaryWriter(log_dir=str(Path(config['paths']['tensorboard']) / run_name))\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    global_step = 0\n",
        "    history = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(config['training']['epochs']):\n",
        "        logger.info(f\"Epoch {epoch+1}/{config['training']['epochs']}\")\n",
        "\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1, global_step = train_epoch(\n",
        "            model, train_loader, optimizer, scheduler, scaler, device, tb_writer, global_step\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, val_prec, val_rec, val_f1, _, _, _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        logger.info(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
        "        logger.info(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        tb_writer.add_scalar('Train/EpochLoss', train_loss, epoch)\n",
        "        tb_writer.add_scalar('Train/Accuracy', train_acc, epoch)\n",
        "        tb_writer.add_scalar('Train/Precision', train_prec, epoch)\n",
        "        tb_writer.add_scalar('Train/Recall', train_rec, epoch)\n",
        "        tb_writer.add_scalar('Train/F1', train_f1, epoch)\n",
        "\n",
        "        tb_writer.add_scalar('Val/Loss', val_loss, epoch)\n",
        "        tb_writer.add_scalar('Val/Accuracy', val_acc, epoch)\n",
        "        tb_writer.add_scalar('Val/Precision', val_prec, epoch)\n",
        "        tb_writer.add_scalar('Val/Recall', val_rec, epoch)\n",
        "        tb_writer.add_scalar('Val/F1', val_f1, epoch)\n",
        "\n",
        "        history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'train_prec': train_prec,\n",
        "            'train_rec': train_rec,\n",
        "            'train_f1': train_f1,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'val_prec': val_prec,\n",
        "            'val_rec': val_rec,\n",
        "            'val_f1': val_f1\n",
        "        })\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            save_path = Path(config['paths']['models']) / run_name\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            checkpoint_name = f\"{run_name}_epoch{epoch+1}_val_loss_{val_loss:.4f}\"\n",
        "            model.save_pretrained(save_path / checkpoint_name)\n",
        "            tokenizer.save_pretrained(save_path / checkpoint_name)\n",
        "\n",
        "            logger.info(f\"Saved best model: {checkpoint_name}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            logger.info(f\"No improvement. Patience: {patience_counter}/{config['early_stopping']['patience']}\")\n",
        "\n",
        "            if patience_counter >= config['early_stopping']['patience']:\n",
        "                logger.info(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    logger.info(f\"Training completed for {run_name} in {training_time/60:.2f} minutes\")\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    history_df = pd.DataFrame(history)\n",
        "    history_df.to_csv(Path(config['paths']['results']) / f\"{run_name}_history.csv\", index=False)\n",
        "\n",
        "    return model, tokenizer, history_df, training_time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Execution & Results\n",
        "\n",
        "This section trains all model configurations:\n",
        "- 2 base models (ELECTRA, RoBERTa)\n",
        "- 2 LoRA ranks (r=4, r=8)\n",
        "- Total: 4 experiments\n",
        "\n",
        "Each model is trained with:\n",
        "- Early stopping (patience=2 was initially used when the **total epochs were 5**, but since the **total epochs were later reduced to 2**, early stopping is now **optional**.\n",
        "- Mixed precision training for efficiency\n",
        "- Gradient clipping for stability\n",
        "- TensorBoard logging for monitoring\n",
        "- Automatic checkpoint saving for best models"
      ],
      "metadata": {
        "id": "TBBVOgZyl6He"
      },
      "id": "TBBVOgZyl6He"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d644356-04f8-4156-9e0c-d9a412524689",
      "metadata": {
        "id": "8d644356-04f8-4156-9e0c-d9a412524689",
        "outputId": "ac714d72-914d-463d-f071-6f9412d966a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23:30:48 | INFO | Starting model training loop...\n",
            "23:30:48 | INFO | ============================================================\n",
            "23:30:48 | INFO | Training: google_electra-base-discriminator_r4\n",
            "23:30:48 | INFO | ============================================================\n",
            "23:30:48 | INFO | Starting training: google_electra-base-discriminator_r4\n",
            "trainable params: 1,255,682 || all params: 110,739,460 || trainable%: 1.1339\n",
            "23:30:49 | INFO | Model loaded: ./local_models/google_electra-base-discriminator with LoRA rank=4, alpha=8\n",
            "23:30:49 | INFO | Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [1:06:38<00:00,  1.04it/s, loss=0.00373, lr=0.000117]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:37<00:00,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00:41:05 | INFO | Train Loss: 0.0882 | Train Acc: 0.9670 | Train F1: 0.9687\n",
            "00:41:05 | INFO | Val Loss: 0.0296 | Val Acc: 0.9921 | Val F1: 0.9925\n",
            "00:41:05 | INFO | Saved best model: google_electra-base-discriminator_r4_epoch1_val_loss_0.0296\n",
            "00:41:05 | INFO | Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [1:06:38<00:00,  1.04it/s, loss=0.000165, lr=0]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:37<00:00,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01:51:21 | INFO | Train Loss: 0.0228 | Train Acc: 0.9942 | Train F1: 0.9945\n",
            "01:51:21 | INFO | Val Loss: 0.0304 | Val Acc: 0.9921 | Val F1: 0.9925\n",
            "01:51:21 | INFO | No improvement. Patience: 1/2\n",
            "01:51:21 | INFO | Training completed for google_electra-base-discriminator_r4 in 140.54 minutes\n",
            "01:51:21 | INFO | Successfully completed training for google_electra-base-discriminator_r4\n",
            "01:51:21 | INFO | ============================================================\n",
            "01:51:21 | INFO | Training: google_electra-base-discriminator_r8\n",
            "01:51:21 | INFO | ============================================================\n",
            "01:51:21 | INFO | Starting training: google_electra-base-discriminator_r8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,919,234 || all params: 111,403,012 || trainable%: 1.7228\n",
            "01:51:21 | INFO | Model loaded: ./local_models/google_electra-base-discriminator with LoRA rank=8, alpha=16\n",
            "01:51:21 | INFO | Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [1:06:31<00:00,  1.05it/s, loss=0.00389, lr=0.000117]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:36<00:00,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "03:01:30 | INFO | Train Loss: 0.0845 | Train Acc: 0.9685 | Train F1: 0.9701\n",
            "03:01:30 | INFO | Val Loss: 0.0277 | Val Acc: 0.9919 | Val F1: 0.9922\n",
            "03:01:30 | INFO | Saved best model: google_electra-base-discriminator_r8_epoch1_val_loss_0.0277\n",
            "03:01:30 | INFO | Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [1:06:31<00:00,  1.05it/s, loss=0.000295, lr=0]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:36<00:00,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "04:11:38 | INFO | Train Loss: 0.0204 | Train Acc: 0.9949 | Train F1: 0.9951\n",
            "04:11:38 | INFO | Val Loss: 0.0285 | Val Acc: 0.9927 | Val F1: 0.9930\n",
            "04:11:38 | INFO | No improvement. Patience: 1/2\n",
            "04:11:38 | INFO | Training completed for google_electra-base-discriminator_r8 in 140.28 minutes\n",
            "04:11:38 | INFO | Successfully completed training for google_electra-base-discriminator_r8\n",
            "04:11:38 | INFO | ============================================================\n",
            "04:11:38 | INFO | Training: FacebookAI_roberta-base_r4\n",
            "04:11:38 | INFO | ============================================================\n",
            "04:11:38 | INFO | Starting training: FacebookAI_roberta-base_r4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,255,682 || all params: 125,902,852 || trainable%: 0.9973\n",
            "04:11:38 | INFO | Model loaded: ./local_models/FacebookAI_roberta-base with LoRA rank=4, alpha=8\n",
            "04:11:38 | INFO | Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [4:22:54<00:00,  3.78s/it, loss=0.000239, lr=0.000117]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:22<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "08:43:55 | INFO | Train Loss: 0.0912 | Train Acc: 0.9659 | Train F1: 0.9680\n",
            "08:43:55 | INFO | Val Loss: 0.0278 | Val Acc: 0.9921 | Val F1: 0.9925\n",
            "08:43:55 | INFO | Saved best model: FacebookAI_roberta-base_r4_epoch1_val_loss_0.0278\n",
            "08:43:55 | INFO | Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [4:22:55<00:00,  3.78s/it, loss=0.000185, lr=0]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:22<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13:16:14 | INFO | Train Loss: 0.0229 | Train Acc: 0.9938 | Train F1: 0.9941\n",
            "13:16:14 | INFO | Val Loss: 0.0314 | Val Acc: 0.9920 | Val F1: 0.9924\n",
            "13:16:14 | INFO | No improvement. Patience: 1/2\n",
            "13:16:14 | INFO | Training completed for FacebookAI_roberta-base_r4 in 544.60 minutes\n",
            "13:16:14 | INFO | Successfully completed training for FacebookAI_roberta-base_r4\n",
            "13:16:14 | INFO | ============================================================\n",
            "13:16:14 | INFO | Training: FacebookAI_roberta-base_r8\n",
            "13:16:14 | INFO | ============================================================\n",
            "13:16:14 | INFO | Starting training: FacebookAI_roberta-base_r8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,919,234 || all params: 126,566,404 || trainable%: 1.5164\n",
            "13:16:14 | INFO | Model loaded: ./local_models/FacebookAI_roberta-base with LoRA rank=8, alpha=16\n",
            "13:16:14 | INFO | Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [4:23:03<00:00,  3.78s/it, loss=0.000282, lr=0.000117]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:23<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17:48:41 | INFO | Train Loss: 0.0888 | Train Acc: 0.9668 | Train F1: 0.9686\n",
            "17:48:41 | INFO | Val Loss: 0.0340 | Val Acc: 0.9914 | Val F1: 0.9918\n",
            "17:48:41 | INFO | Saved best model: FacebookAI_roberta-base_r8_epoch1_val_loss_0.0340\n",
            "17:48:41 | INFO | Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [4:23:04<00:00,  3.78s/it, loss=0.00215, lr=0]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:23<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:21:09 | INFO | Train Loss: 0.0203 | Train Acc: 0.9949 | Train F1: 0.9952\n",
            "22:21:09 | INFO | Val Loss: 0.0301 | Val Acc: 0.9930 | Val F1: 0.9934\n",
            "22:21:09 | INFO | Saved best model: FacebookAI_roberta-base_r8_epoch2_val_loss_0.0301\n",
            "22:21:09 | INFO | Training completed for FacebookAI_roberta-base_r8 in 544.92 minutes\n",
            "22:21:09 | INFO | Successfully completed training for FacebookAI_roberta-base_r8\n",
            "22:21:09 | INFO | ============================================================\n",
            "22:21:09 | INFO | All model training completed!\n",
            "22:21:09 | INFO | ============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Starting model training loop...\")\n",
        "\n",
        "all_results = {}\n",
        "training_times = {}\n",
        "\n",
        "for model_name in config['models']['names']:\n",
        "    for lora_rank in config['lora']['ranks']:\n",
        "        run_name = f\"{model_name.split('/')[-1]}_r{lora_rank}\"\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(f\"Training: {run_name}\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            train_dataset = SpamDataset(\n",
        "                train_df[config['data']['text_column']].values,\n",
        "                train_df[config['data']['label_column']].values,\n",
        "                tokenizer,\n",
        "                config['models']['max_length']\n",
        "            )\n",
        "\n",
        "            val_dataset = SpamDataset(\n",
        "                val_df[config['data']['text_column']].values,\n",
        "                val_df[config['data']['label_column']].values,\n",
        "                tokenizer,\n",
        "                config['models']['max_length']\n",
        "            )\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=config['training']['batch_size'],\n",
        "                shuffle=True,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=config['training']['batch_size'],\n",
        "                shuffle=False,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            model, tokenizer, history, training_time = train_model(\n",
        "                model_name,\n",
        "                lora_rank,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "            all_results[run_name] = {\n",
        "                'model': model,\n",
        "                'tokenizer': tokenizer,\n",
        "                'history': history\n",
        "            }\n",
        "            training_times[run_name] = training_time\n",
        "\n",
        "            logger.info(f\"Successfully completed training for {run_name}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e) and config['training']['auto_reduce_batch_size']:\n",
        "                logger.warning(f\"OOM error for {run_name}. Attempting with reduced batch size...\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                reduced_batch_size = max(\n",
        "                    config['training']['batch_size'] // 2,\n",
        "                    config['training']['min_batch_size']\n",
        "                )\n",
        "\n",
        "                if reduced_batch_size >= config['training']['min_batch_size']:\n",
        "                    logger.info(f\"Retrying with batch_size={reduced_batch_size}\")\n",
        "                    train_loader = DataLoader(\n",
        "                        train_dataset,\n",
        "                        batch_size=reduced_batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=0\n",
        "                    )\n",
        "                    val_loader = DataLoader(\n",
        "                        val_dataset,\n",
        "                        batch_size=reduced_batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0\n",
        "                    )\n",
        "\n",
        "                    model, tokenizer, history, training_time = train_model(\n",
        "                        model_name,\n",
        "                        lora_rank,\n",
        "                        train_loader,\n",
        "                        val_loader,\n",
        "                        DEVICE\n",
        "                    )\n",
        "\n",
        "                    all_results[run_name] = {\n",
        "                        'model': model,\n",
        "                        'tokenizer': tokenizer,\n",
        "                        'history': history\n",
        "                    }\n",
        "                    training_times[run_name] = training_time\n",
        "                else:\n",
        "                    logger.error(f\"Cannot reduce batch size below minimum for {run_name}\")\n",
        "                    raise\n",
        "            else:\n",
        "                logger.error(f\"Training failed for {run_name}: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"All model training completed!\")\n",
        "logger.info(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f847a93b-1e3f-4888-9849-486f550f4999",
      "metadata": {
        "id": "f847a93b-1e3f-4888-9849-486f550f4999",
        "outputId": "091820b7-202e-49f8-a697-78d32fe67539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'google_electra-base-discriminator_r4': {'model': PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): ElectraForSequenceClassification(\n",
            "      (electra): ElectraModel(\n",
            "        (embeddings): ElectraEmbeddings(\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): ElectraEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0-11): 12 x ElectraLayer(\n",
            "              (attention): ElectraAttention(\n",
            "                (self): ElectraSelfAttention(\n",
            "                  (query): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (key): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (value): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): ElectraSelfOutput(\n",
            "                  (dense): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): ElectraIntermediate(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=4, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): ElectraOutput(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3072, out_features=4, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (classifier): ModulesToSaveWrapper(\n",
            "        (original_module): ElectraClassificationHead(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): GELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "        )\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): ElectraClassificationHead(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "), 'tokenizer': ElectraTokenizerFast(name_or_path='./local_models/google_electra-base-discriminator', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "), 'history':    epoch  train_loss  train_acc  train_prec  train_rec  train_f1  val_loss  \\\n",
            "0      1    0.088222   0.967045    0.969729   0.967576  0.968651  0.029563   \n",
            "1      2    0.022767   0.994173    0.993018   0.995929  0.994471  0.030362   \n",
            "\n",
            "    val_acc  val_prec   val_rec    val_f1  \n",
            "0  0.992091  0.994286  0.990663  0.992471  \n",
            "1  0.992091  0.994512  0.990435  0.992469  }, 'google_electra-base-discriminator_r8': {'model': PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): ElectraForSequenceClassification(\n",
            "      (electra): ElectraModel(\n",
            "        (embeddings): ElectraEmbeddings(\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): ElectraEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0-11): 12 x ElectraLayer(\n",
            "              (attention): ElectraAttention(\n",
            "                (self): ElectraSelfAttention(\n",
            "                  (query): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (key): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (value): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): ElectraSelfOutput(\n",
            "                  (dense): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): ElectraIntermediate(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): ElectraOutput(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (classifier): ModulesToSaveWrapper(\n",
            "        (original_module): ElectraClassificationHead(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): GELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "        )\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): ElectraClassificationHead(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "), 'tokenizer': ElectraTokenizerFast(name_or_path='./local_models/google_electra-base-discriminator', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "), 'history':    epoch  train_loss  train_acc  train_prec  train_rec  train_f1  val_loss  \\\n",
            "0      1    0.084497   0.968528    0.971208   0.968914  0.970060  0.027712   \n",
            "1      2    0.020407   0.994862    0.993614   0.996641  0.995125  0.028501   \n",
            "\n",
            "    val_acc  val_prec   val_rec    val_f1  \n",
            "0  0.991851  0.994057  0.990435  0.992243  \n",
            "1  0.992690  0.994744  0.991346  0.993042  }, 'FacebookAI_roberta-base_r4': {'model': PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): RobertaForSequenceClassification(\n",
            "      (roberta): RobertaModel(\n",
            "        (embeddings): RobertaEmbeddings(\n",
            "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "          (token_type_embeddings): Embedding(1, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): RobertaEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0-11): 12 x RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSdpaSelfAttention(\n",
            "                  (query): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (key): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (value): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=4, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3072, out_features=4, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (classifier): ModulesToSaveWrapper(\n",
            "        (original_module): RobertaClassificationHead(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "        )\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): RobertaClassificationHead(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "), 'tokenizer': RobertaTokenizerFast(name_or_path='./local_models/FacebookAI_roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "), 'history':    epoch  train_loss  train_acc  train_prec  train_rec  train_f1  val_loss  \\\n",
            "0      1     0.09120   0.965892    0.955973   0.980329  0.967998  0.027815   \n",
            "1      2     0.02291   0.993813    0.992481   0.995787  0.994131  0.031359   \n",
            "\n",
            "    val_acc  val_prec   val_rec    val_f1  \n",
            "0  0.992091  0.992709  0.992257  0.992483  \n",
            "1  0.991971  0.994511  0.990207  0.992354  }, 'FacebookAI_roberta-base_r8': {'model': PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): RobertaForSequenceClassification(\n",
            "      (roberta): RobertaModel(\n",
            "        (embeddings): RobertaEmbeddings(\n",
            "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "          (token_type_embeddings): Embedding(1, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): RobertaEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0-11): 12 x RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSdpaSelfAttention(\n",
            "                  (query): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (key): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (value): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): lora.Linear(\n",
            "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.1, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                    (lora_magnitude_vector): ModuleDict()\n",
            "                  )\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (classifier): ModulesToSaveWrapper(\n",
            "        (original_module): RobertaClassificationHead(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "        )\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): RobertaClassificationHead(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "), 'tokenizer': RobertaTokenizerFast(name_or_path='./local_models/FacebookAI_roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "), 'history':    epoch  train_loss  train_acc  train_prec  train_rec  train_f1  val_loss  \\\n",
            "0      1    0.088832   0.966835    0.964231   0.973070  0.968630  0.034006   \n",
            "1      2    0.020286   0.994922    0.993699   0.996669  0.995182  0.030141   \n",
            "\n",
            "    val_acc  val_prec   val_rec    val_f1  \n",
            "0  0.991372  0.994504  0.989069  0.991779  \n",
            "1  0.993050  0.994522  0.992257  0.993388  }}\n"
          ]
        }
      ],
      "source": [
        "all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Model Evaluation & Comparison\n",
        "\n",
        "This section performs comprehensive evaluation on the test set:\n",
        "- **Core Metrics**: Accuracy, Precision, Recall, F1-Score\n",
        "- **Probabilistic Metrics**: ROC-AUC, PR-AUC\n",
        "- **Confusion Matrices**: Detailed error analysis for each model\n",
        "- **Classification Reports**: Per-class performance breakdown\n",
        "- **Comparative Visualizations**:\n",
        "  - Metrics comparison across all models\n",
        "  - ROC curves comparison\n",
        "  - Precision-Recall curves comparison\n",
        "- **LoRA Ablation Study**: Analysis of rank 4 vs rank 8 performance and parameter efficiency"
      ],
      "metadata": {
        "id": "1P3ENEMJmAAA"
      },
      "id": "1P3ENEMJmAAA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2fe2d2-7d32-4f4c-bbab-ab282d2a62af",
      "metadata": {
        "id": "5e2fe2d2-7d32-4f4c-bbab-ab282d2a62af",
        "outputId": "ef9a3bc4-f5f0-46c6-aa2e-af47fca561ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:21:57 | INFO | Evaluating all models on test set...\n",
            "22:21:57 | INFO | Evaluating google_electra-base-discriminator_r4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:38<00:00,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:25:35 | INFO | google_electra-base-discriminator_r4 - Acc: 0.9939, Prec: 0.9945, Rec: 0.9939, F1: 0.9942, ROC-AUC: 0.9993\n",
            "22:25:35 | INFO | Evaluating google_electra-base-discriminator_r8...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [03:37<00:00,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:29:13 | INFO | google_electra-base-discriminator_r8 - Acc: 0.9935, Prec: 0.9952, Rec: 0.9925, F1: 0.9938, ROC-AUC: 0.9993\n",
            "22:29:13 | INFO | Evaluating FacebookAI_roberta-base_r4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:23<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:38:36 | INFO | FacebookAI_roberta-base_r4 - Acc: 0.9941, Prec: 0.9950, Rec: 0.9939, F1: 0.9944, ROC-AUC: 0.9990\n",
            "22:38:36 | INFO | Evaluating FacebookAI_roberta-base_r8...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 522/522 [09:23<00:00,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:48:00 | INFO | FacebookAI_roberta-base_r8 - Acc: 0.9945, Prec: 0.9952, Rec: 0.9943, F1: 0.9948, ROC-AUC: 0.9989\n",
            "22:48:00 | INFO | Saved evaluation results to model_comparison.csv\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON - TEST SET RESULTS\n",
            "================================================================================\n",
            "                               model  accuracy  precision   recall  f1_score  roc_auc   pr_auc  training_time_min  trainable_params  total_params\n",
            "google_electra-base-discriminator_r4  0.993889   0.994531 0.993851  0.994191 0.999311 0.999237         140.538083           1255682     110739460\n",
            "google_electra-base-discriminator_r8  0.993529   0.995204 0.992485  0.993843 0.999258 0.999168         140.275087           1919234     111403012\n",
            "          FacebookAI_roberta-base_r4  0.994128   0.994984 0.993851  0.994417 0.999009 0.998843         544.596072           1255682     125902852\n",
            "          FacebookAI_roberta-base_r8  0.994488   0.995213 0.994307  0.994760 0.998905 0.998683         544.919773           1919234     126566404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Evaluating all models on test set...\")\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for run_name, result in all_results.items():\n",
        "    logger.info(f\"Evaluating {run_name}...\")\n",
        "\n",
        "    model = result['model']\n",
        "    tokenizer = result['tokenizer']\n",
        "\n",
        "    test_dataset = SpamDataset(\n",
        "        test_df[config['data']['text_column']].values,\n",
        "        test_df[config['data']['label_column']].values,\n",
        "        tokenizer,\n",
        "        config['models']['max_length']\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['training']['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc, test_prec, test_rec, test_f1, predictions, true_labels, probs = evaluate(\n",
        "        model, test_loader, DEVICE\n",
        "    )\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(true_labels, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(true_labels, probs)\n",
        "    pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'model': run_name,\n",
        "        'accuracy': test_acc,\n",
        "        'precision': test_prec,\n",
        "        'recall': test_rec,\n",
        "        'f1_score': test_f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'training_time_min': training_times[run_name] / 60,\n",
        "        'trainable_params': trainable_params,\n",
        "        'total_params': total_params,\n",
        "        'predictions': predictions,\n",
        "        'true_labels': true_labels,\n",
        "        'probs': probs,\n",
        "        'confusion_matrix': cm,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'precision_curve': precision_curve,\n",
        "        'recall_curve': recall_curve\n",
        "    })\n",
        "\n",
        "    logger.info(f\"{run_name} - Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame([{\n",
        "    'model': r['model'],\n",
        "    'accuracy': r['accuracy'],\n",
        "    'precision': r['precision'],\n",
        "    'recall': r['recall'],\n",
        "    'f1_score': r['f1_score'],\n",
        "    'roc_auc': r['roc_auc'],\n",
        "    'pr_auc': r['pr_auc'],\n",
        "    'training_time_min': r['training_time_min'],\n",
        "    'trainable_params': r['trainable_params'],\n",
        "    'total_params': r['total_params']\n",
        "} for r in evaluation_results])\n",
        "\n",
        "results_df.to_csv(Path(config['paths']['results']) / 'model_comparison.csv', index=False)\n",
        "logger.info(\"Saved evaluation results to model_comparison.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON - TEST SET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b961ca-fa60-4632-8f96-6a4b92f29d5c",
      "metadata": {
        "id": "c3b961ca-fa60-4632-8f96-6a4b92f29d5c",
        "outputId": "488d60ae-1e87-4361-dbaf-0d478551bd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:48:00 | INFO | Creating individual evaluation visualizations...\n",
            "22:48:02 | INFO | Saved plot: visualizations/results/google_electra-base-discriminator_r4_evaluation.png\n",
            "\n",
            "google_electra-base-discriminator_r4 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99      3954\n",
            "        Spam       0.99      0.99      0.99      4391\n",
            "\n",
            "    accuracy                           0.99      8345\n",
            "   macro avg       0.99      0.99      0.99      8345\n",
            "weighted avg       0.99      0.99      0.99      8345\n",
            "\n",
            "22:48:03 | INFO | Saved plot: visualizations/results/google_electra-base-discriminator_r8_evaluation.png\n",
            "\n",
            "google_electra-base-discriminator_r8 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99      3954\n",
            "        Spam       1.00      0.99      0.99      4391\n",
            "\n",
            "    accuracy                           0.99      8345\n",
            "   macro avg       0.99      0.99      0.99      8345\n",
            "weighted avg       0.99      0.99      0.99      8345\n",
            "\n",
            "22:48:04 | INFO | Saved plot: visualizations/results/FacebookAI_roberta-base_r4_evaluation.png\n",
            "\n",
            "FacebookAI_roberta-base_r4 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99      3954\n",
            "        Spam       0.99      0.99      0.99      4391\n",
            "\n",
            "    accuracy                           0.99      8345\n",
            "   macro avg       0.99      0.99      0.99      8345\n",
            "weighted avg       0.99      0.99      0.99      8345\n",
            "\n",
            "22:48:05 | INFO | Saved plot: visualizations/results/FacebookAI_roberta-base_r8_evaluation.png\n",
            "\n",
            "FacebookAI_roberta-base_r8 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99      3954\n",
            "        Spam       1.00      0.99      0.99      4391\n",
            "\n",
            "    accuracy                           0.99      8345\n",
            "   macro avg       0.99      0.99      0.99      8345\n",
            "weighted avg       0.99      0.99      0.99      8345\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Creating individual evaluation visualizations...\")\n",
        "\n",
        "for result in evaluation_results:\n",
        "    run_name = result['model']\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "    cm = result['confusion_matrix']\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "    axes[0, 0].set_ylabel('True Label')\n",
        "    axes[0, 0].set_xlabel('Predicted Label')\n",
        "    axes[0, 0].set_title(f'{run_name} - Confusion Matrix (Counts)')\n",
        "\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[0, 1], xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "    axes[0, 1].set_ylabel('True Label')\n",
        "    axes[0, 1].set_xlabel('Predicted Label')\n",
        "    axes[0, 1].set_title(f'{run_name} - Confusion Matrix (Normalized)')\n",
        "\n",
        "    axes[1, 0].plot(result['fpr'], result['tpr'], linewidth=2, label=f'ROC (AUC = {result[\"roc_auc\"]:.3f})')\n",
        "    axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "    axes[1, 0].set_xlabel('False Positive Rate')\n",
        "    axes[1, 0].set_ylabel('True Positive Rate')\n",
        "    axes[1, 0].set_title(f'{run_name} - ROC Curve')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].plot(result['recall_curve'], result['precision_curve'], linewidth=2, label=f'PR (AUC = {result[\"pr_auc\"]:.3f})')\n",
        "    axes[1, 1].set_xlabel('Recall')\n",
        "    axes[1, 1].set_ylabel('Precision')\n",
        "    axes[1, 1].set_title(f'{run_name} - Precision-Recall Curve')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_plot(f'{run_name}_evaluation.png', subdir='results')\n",
        "\n",
        "    print(f\"\\n{run_name} Classification Report:\")\n",
        "    print(classification_report(result['true_labels'], result['predictions'], target_names=['Ham', 'Spam']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcdd1af2-fa7a-45f8-8e48-0ca1b3a71a87",
      "metadata": {
        "id": "dcdd1af2-fa7a-45f8-8e48-0ca1b3a71a87",
        "outputId": "24a2610b-fea9-4592-bda9-19af99d64711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:48:05 | INFO | Creating comparison visualizations...\n",
            "22:48:06 | INFO | Saved plot: visualizations/results/metrics_comparison.png\n",
            "22:48:07 | INFO | Saved plot: visualizations/results/roc_curves_comparison.png\n",
            "22:48:07 | INFO | Saved plot: visualizations/results/pr_curves_comparison.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/results/pr_curves_comparison.png'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Creating comparison visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'pr_auc']\n",
        "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'PR-AUC']\n",
        "\n",
        "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "\n",
        "    ax = axes[row, col]\n",
        "    values = results_df[metric].values\n",
        "    models = results_df['model'].values\n",
        "\n",
        "    colors = []\n",
        "    for model in models:\n",
        "        if 'google' in model:\n",
        "            colors.append('#2E5EAA' if 'r8' in model else '#5A8FCC')\n",
        "        elif 'roberta' in model:\n",
        "            colors.append('#2D8E4D' if 'r8' in model else '#52B36F')\n",
        "        else:\n",
        "            colors.append('#E67E22' if 'r8' in model else '#F39C44')\n",
        "\n",
        "    bars = ax.bar(range(len(models)), values, color=colors)\n",
        "    ax.set_xticks(range(len(models)))\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.set_ylabel(name)\n",
        "    ax.set_title(f'{name} Comparison')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('metrics_comparison.png', subdir='results')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "for result in evaluation_results:\n",
        "    model_base = result['model'].split('_')[0]\n",
        "    lora_r = result['model'].split('_')[1]\n",
        "    linestyle = '-' if 'r8' in lora_r else '--'\n",
        "\n",
        "    if 'google' in model_base:\n",
        "        color = '#2E5EAA'\n",
        "    elif 'roberta' in model_base:\n",
        "        color = '#2D8E4D'\n",
        "    else:\n",
        "        color = '#E67E22'\n",
        "\n",
        "    ax.plot(result['fpr'], result['tpr'], linestyle=linestyle, linewidth=2, color=color, label=f\"{result['model']} (AUC={result['roc_auc']:.3f})\")\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
        "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
        "ax.set_title('ROC Curves - All Models Comparison', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('roc_curves_comparison.png', subdir='results')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "for result in evaluation_results:\n",
        "    model_base = result['model'].split('_')[0]\n",
        "    lora_r = result['model'].split('_')[1]\n",
        "    linestyle = '-' if 'r8' in lora_r else '--'\n",
        "\n",
        "    if 'google' in model_base:\n",
        "        color = '#2E5EAA'\n",
        "    elif 'roberta' in model_base:\n",
        "        color = '#2D8E4D'\n",
        "    else:\n",
        "        color = '#E67E22'\n",
        "\n",
        "    ax.plot(result['recall_curve'], result['precision_curve'], linestyle=linestyle,\n",
        "            linewidth=2, color=color, label=f\"{result['model']} (AUC={result['pr_auc']:.3f})\")\n",
        "\n",
        "ax.set_xlabel('Recall', fontsize=12)\n",
        "ax.set_ylabel('Precision', fontsize=12)\n",
        "ax.set_title('Precision-Recall Curves - All Models Comparison', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('pr_curves_comparison.png', subdir='results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d97ba06-76a2-4bb2-9284-7a1ad2d19186",
      "metadata": {
        "id": "1d97ba06-76a2-4bb2-9284-7a1ad2d19186",
        "outputId": "fe740904-fd30-49d5-a14d-3fcb79b8376d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22:52:25 | INFO | Performing LoRA ablation analysis...\n",
            "\n",
            "============================================================\n",
            "LoRA ABLATION STUDY (r=4 vs r=8)\n",
            "============================================================\n",
            "                            model    r4_f1    r8_f1   f1_diff  r4_precision  r8_precision  r4_recall  r8_recall  r4_params  r8_params  params_ratio\n",
            "          FacebookAI_roberta-base 0.994417 0.994760  0.000342      0.994984      0.995213   0.993851   0.994307    1255682    1919234       1.52844\n",
            "google_electra-base-discriminator 0.994191 0.993843 -0.000348      0.994531      0.995204   0.993851   0.992485    1255682    1919234       1.52844\n",
            "22:52:26 | INFO | Saved plot: visualizations/results/lora_ablation_analysis.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'visualizations/results/lora_ablation_analysis.png'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logger.info(\"Performing LoRA ablation analysis...\")\n",
        "\n",
        "base_models = set([r['model'].rsplit('_', 1)[0] for r in evaluation_results])\n",
        "\n",
        "ablation_data = []\n",
        "for base_model in base_models:\n",
        "    r4_result = next((r for r in evaluation_results if r['model'] == f\"{base_model}_r4\"), None)\n",
        "    r8_result = next((r for r in evaluation_results if r['model'] == f\"{base_model}_r8\"), None)\n",
        "\n",
        "    if r4_result and r8_result:\n",
        "        ablation_data.append({\n",
        "            'model': base_model,\n",
        "            'r4_f1': r4_result['f1_score'],\n",
        "            'r8_f1': r8_result['f1_score'],\n",
        "            'f1_diff': r8_result['f1_score'] - r4_result['f1_score'],\n",
        "            'r4_precision': r4_result['precision'],\n",
        "            'r8_precision': r8_result['precision'],\n",
        "            'r4_recall': r4_result['recall'],\n",
        "            'r8_recall': r8_result['recall'],\n",
        "            'r4_params': r4_result['trainable_params'],\n",
        "            'r8_params': r8_result['trainable_params'],\n",
        "            'params_ratio': r8_result['trainable_params'] / r4_result['trainable_params']\n",
        "        })\n",
        "\n",
        "ablation_df = pd.DataFrame(ablation_data)\n",
        "ablation_df.to_csv(Path(config['paths']['results']) / 'lora_ablation.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LoRA ABLATION STUDY (r=4 vs r=8)\")\n",
        "print(\"=\"*60)\n",
        "print(ablation_df.to_string(index=False))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=tuple(config['visualization']['figure_sizes']['large']))\n",
        "\n",
        "x = np.arange(len(ablation_df))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, ablation_df['r4_f1'], width, label='r=4', alpha=0.8)\n",
        "axes[0].bar(x + width/2, ablation_df['r8_f1'], width, label='r=8', alpha=0.8)\n",
        "axes[0].set_xlabel('Model')\n",
        "axes[0].set_ylabel('F1 Score')\n",
        "axes[0].set_title('F1 Score: LoRA r=4 vs r=8')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(ablation_df['model'], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].bar(ablation_df['model'], ablation_df['f1_diff'], color=['green' if d > 0 else 'red' for d in ablation_df['f1_diff']])\n",
        "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[1].set_xlabel('Model')\n",
        "axes[1].set_ylabel('F1 Difference (r=8 - r=4)')\n",
        "axes[1].set_title('Performance Gain from r=8')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[2].bar(ablation_df['model'], ablation_df['params_ratio'])\n",
        "axes[2].axhline(y=1, color='red', linestyle='--', linewidth=1)\n",
        "axes[2].set_xlabel('Model')\n",
        "axes[2].set_ylabel('Parameter Ratio (r=8 / r=4)')\n",
        "axes[2].set_title('Trainable Parameters Increase')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot('lora_ablation_analysis.png', subdir='results')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Conclusion & Key Findings\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "Based on comprehensive evaluation across 4 model configurations:\n",
        "\n",
        "**Best Performing Model:**\n",
        "- **RoBERTa-base with LoRA rank 8** achieved the highest test accuracy (99.45%) and F1 (99.48%).\n",
        "- **Note:** All four models achieved highly competitive F1 scores (~99.38%).\n",
        "\n",
        "**Key Insights:**\n",
        "1. **LoRA Efficiency**: All models achieved over 99% accuracy with only 1.2M to 1.9M trainable parameters (a massive efficiency gain over full fine-tuning).\n",
        "2. **Speed vs. Accuracy Trade-off**: The top-performing RoBERTa-r8 is significantly slower in inference (9.4 minutes) compared to ELECTRA-r4 (3.6 minutes).\n",
        "3. **Optimized Choice**: **google_electra-base-discriminator_r4** delivers a nearly identical F1 score (99.42%) but with the **fastest inference time** and the **lowest trainable parameter count** (1.2M).\n",
        "4. **Rank Comparison**: Rank 8 provided only marginal F1 improvements (~0.04-0.06% F1) over Rank 4, making Rank 4 the more parameter-efficient choice.\n",
        "\n",
        "**Production Recommendation:**\n",
        "**google_electra-base-discriminator_r4** offers the optimal balance of high performance (F1 99.42%) and operational efficiency (fastest inference, lowest trainable parameters) for deployment.\n",
        "\n",
        "**Next Steps:**\n",
        "- Deploy the **google_electra-base-discriminator_r4** model with threshold tuning for production precision targets\n",
        "- Monitor performance on live data for concept drift\n",
        "- Periodic retraining as spam tactics evolve"
      ],
      "metadata": {
        "id": "wR68TGi_pMG_"
      },
      "id": "wR68TGi_pMG_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10f3faa-1e7a-445f-9c4b-7b84a9759744",
      "metadata": {
        "scrolled": true,
        "id": "b10f3faa-1e7a-445f-9c4b-7b84a9759744",
        "outputId": "0246b395-e106-4a83-dc95-3ec3048afeb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In',\n",
              " 'Out',\n",
              " '_',\n",
              " '__',\n",
              " '___',\n",
              " '__builtin__',\n",
              " '__builtins__',\n",
              " '__doc__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__session__',\n",
              " '__spec__',\n",
              " '_dh',\n",
              " '_i',\n",
              " '_i1',\n",
              " '_ih',\n",
              " '_ii',\n",
              " '_iii',\n",
              " '_oh',\n",
              " 'exit',\n",
              " 'get_ipython',\n",
              " 'open',\n",
              " 'quit']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b899d0c6-28e5-4074-bdae-c9e35aff3015",
      "metadata": {
        "id": "b899d0c6-28e5-4074-bdae-c9e35aff3015"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_spam_classifier",
      "language": "python",
      "name": "venv_spam_classifier"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YBe4o2GcjvUj",
        "NV_4hk5ck5sO",
        "ByG4XlqlpYvc",
        "eFtbWtLVk-St",
        "-zOAPlG3lLEQ",
        "pXqlHdktlUA0",
        "TBBVOgZyl6He",
        "1P3ENEMJmAAA"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}